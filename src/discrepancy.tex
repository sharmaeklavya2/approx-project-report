\section{Discrepancy Minimization}
A fundamental problem in combinatorics is minimizing the discrepancy of a set system. 
Given a set system $\paren{\mathcal{S} = \set{S_1,S_2,\hdots,S_m}}$, formed by taking subsets of universe $\paren{V = \set{1,2,\hdots,n}}$, we define a coloring $\chi: V \to \set{-1,+1}$.
The minimum discrepancy of the set system $\paren{\text{disc}\paren{\mathcal{S}}}$ is defined as 
\[ \text{disc}\paren{\mathcal{S}} = \min_{\chi}\max_{S \in \mathcal{S}}{\abs{\sum_{i \in S}{\chi(i)}}} \]
This can be generalised to define discrepancy for arbitrary real matrices $A$ (here we consider $A$ as the incidence matrix of $\paren{V,S}$) as 
\[ \text{disc}(A) = \min_{x \in \set{-1,+1}^n} \norm{Ax}_{\infty}\]
For a random coloring, the resulting discrepancy can be shown to be bounded by $3\sqrt{n\log m}$ using Chernoff and Union bounds.
In \cite{Spencer1985}, Spencer showed that for any set system $\mathcal{S}$, there exists a coloring $\chi$ such that $\text{disc}(\mathcal{S}) < K\sqrt{n\;\log_2{(m/n)}}$.
For $m=n$ (our regime of interest from now onwards), we can set $K=6$ and we get a significant improvement compared to random coloring.
In fact, this is tight if we consider the incidence matrix of $\paren{V,S}$ to be a Hadamard matrix. The proof is non-constructive and it was conjectured by Spencer that no efficient algorithm exists for finding such a coloring.

In a breakthrough result \cite{bansal10cadm}, Nikhil Bansal gave the first polynomial time randomized algorithm which gives a coloring with discrepancy $O(\sqrt{n})$.
The algorithm is based on SDP relaxation of the problem and rounding using random walks .
Shachar Lovett and Raghu Meka \cite{12lovettmeka} gave a simpler randomized polynomial time algorithm based on basic linear algebra and a restricted form of random walk called edge walk.
We discuss the result by Lovett and Meka here.

We state a few properties about Gaussians (without proof).
\begin{Claim} \label{gaussian_properties}
Let $\mathcal{N}\paren{\mu,{\sigma}^2}$ denote a Gaussian distribution with mean $\mu$ and variance ${\sigma}^2$. Let $\set{v_1,v_2,\hdots,v_d}$ be an orthonormal basis for a linear subspace $V \subseteq \mathbb{R}^n$. 
The standard multi-dimensional Gaussian supported on $V$ is given by $G \sim \mathcal{N}(V) = G_1v_1 + \hdots + G_dv_d$ where $G_1,G_2,\hdots,G_d \sim \mathcal{N}(0,1)$
(it can be proven that the distribution of $G$ does not dependent on the choice of the orthonormal basis).
We have that,\\
(i) For all $u \in \mathbb{R}^n, \inprod{G,u} \sim \mathcal{N}(0,{\sigma}^2)$ where ${\sigma}^2 \leq \norm{u}^2$\\
(ii) Let $\inprod{G,e_i} \sim \mathcal{N}(0,{\sigma_i}^2)$.
Then $\sum_{i=1}^{n}{\sigma_i}^2 = \text{dim}(V)$\\
(iii) For $G \sim \mathcal{N}(0,1), \lambda >0, Pr\left[\abs{G}>\lambda\right] \leq 2e^{-{\lambda}^2/2} $\\
(iv) Let $\set{X_i}_{i=1}^{T}$ and $\set{Y_i}_{i=1}^{T}$ be a set of random variables such that $Y_i$ is a function of $X_i$. If $Y_i|\paren{X_1 = x_1,X_2=x_2,\hdots,X_{i-1}=x_{i-1}}$ is a Gaussian with 0 mean and variance at most 1, for all $1 \leq i \leq T,x_1,\hdots,x_{i-1}\in \mathbb{R}$ and for $\lambda > 0$, then
\[ Pr\left[\abs{Y_1 + \hdots + Y_T} \geq \lambda\sqrt{T}\right] \leq 2e^{-{\lambda}^2/2} \]
\end{Claim}

The algorithm rests on a Partial Coloring Lemma \ref{pcl}. Partial Coloring Method was developed by Beck \cite{Beck1981} by relaxing the discrepancy vector to $\set{-1,0,+1}$. Here it is further relaxed to $\left[-1,+1\right]$. 
We denote the relaxed discrepancy vector by $x$.

\begin{lemma} [Partial Coloring Lemma]\label{pcl}
Let $v_1,\hdots,v_m \in \mathbb{R}^n$ be vectors and $c_1,\hdots,c_m \geq 0$ be constants such that $\sum_{j=1}^{n}e^{{-c_j}^2/16} \leq n/16$ and $\delta > 0$ be a small approximation parameter.
There exists a randomized algorithm running in time $O\paren{(m+n)^3{\delta}^{-2}\log(mn/\delta)}$ which starting at $x_0 \in [-1,1]^n$ returns the vector $x \in [-1,1]^n$ with probability at least $0.1$, such that\\
\qquad (i) $\abs{\inprod{x-x_0,v_j}} \leq c_j\norm{v_j}$ \qquad \qquad \qquad \qquad
(ii) $\abs{\set{i: \abs{x_i} \geq 1- \delta}} \geq n/2$
\end{lemma}
We normalize all $\norm{v_i}=1$ and consider the polytope $\mathcal{P}$ for feasible $x$ as 
\[ \mathcal{P} = \set{x\in \mathbb{R}^n : \abs{x_i} \leq 1 \wedge \abs{\inprod{x-x_0,v_j}} \leq c_j }\]
where the constraints are called variable constraints and discrepancy constraints respectively.
We consider a constrained random walk in $\mathcal{P}$ called Edge-walk starting at $x=x_0$ and run it for $T$ steps. Let $\gamma >0$ be the step size of the walk such that $\delta = O(\gamma \sqrt{\log(nm/\gamma)}) < 0.1$. For $t= 1,\hdots,T$ we define,\\
(i) $C_t^{var} := C_t^{var}\paren{X_{t-1}} = \set{ i \in [n] : \abs{\paren{X_{t-1}}_i} \geq 1- \delta}$ \\
(ii) $C_t^{disc} := C_t^{disc}\paren{X_{t-1}} = \set{j \in [m] : \abs{\inprod{X_{t-1}-x_0,v_j}} \geq c_j - \delta}$ \\
(iii) $\mathcal{V}_t := \mathcal{V}\paren{X_{t-1}} = \set{u \in \mathbb{R}^n : \paren{u_i = 0, \forall i \in C_t^{var}} \wedge \paren{\inprod{u,v_j}=0, \forall j \in C_t^{disc}} }$  \\
(iv) $X_t = X_{t-1} + \gamma U_t$, where $U_t \sim \mathcal{N}\paren{\mathcal{V}_t}$

$C_t^{var}$ and $C_t^{disc}$ denote almost satisfied variable and discrepancy constraints and $\mathcal{V}_t$ is the subspace orthogonal to the constraints that have been satisfied.
We first observe that the update can only shrink the null space $\mathcal{V}_t$ at each iteration.
\begin{Claim}
$C_t^{var} \subseteq C_{t+1}^{var}$, $C_t^{disc} \subseteq C_{t+1}^{disc}$ and $dim(\mathcal{V}_t) \geq dim(\mathcal{V}_{t+1})$ $\forall$ $1 \leq t \leq T$
\end{Claim}
Next we claim that this edge walk remains inside $\mathcal{P}$ with high probability.
\begin{Claim}
For a small step size $\gamma \leq \delta/\sqrt{C\log(mn/\gamma)}$ and large constant $C$, the probability that the walk never leaves $\mathcal{P}$ $\paren{X_0,\hdots,X_T \in \mathcal{P}}$ is at least $1 - 1/(mn)^{C-2}$.
\end{Claim}
\begin{proof}
Let $E_t$ be the event that the walk leaves $\mathcal{P}$ for the first time. 
For this to happen, one of the constraints has to be violated, say $(X_t)_i>1$.
This implies that $(X_{t-1})_i \leq 1 - \delta$ as otherwise $i \in C_t^{var}$ and $(U_t)_i=0$ and $(X_{t})_i=(X_{t-1})_i$. 
The above holds if $\abs{(U_t)_i} \geq \delta/\gamma$ which implies that $\abs{\inprod{U_t,w}} \geq \delta/\gamma$ for some $w \in \mathbb{R}^n$.
Since $U_t \sim \mathcal{N}(\mathcal{V}_t)$, by Claim \ref{gaussian_properties}, $\inprod{U_t,w}$ is a standard Gaussian. Invoking the tail bound in Claim \ref{gaussian_properties} gives us the desired result.
\end{proof}

Next we show that on an average, only a few discrepancy constraints are satisfied.
\begin{Claim}
\label{thm:less-disc-constraints}
$\mathbb{E}\left[\abs{C_T^{disc}} < n/8 \right]$
\end{Claim}
\begin{proof}
We define $Y_t = c_j\inprod{\frac{v_j}{\norm{v_j}},U_t}$. $Y_t$ is a Gaussian with mean 0 and variance at most $\norm{u}^2 = {c_j}^2$ and a function of $U_t$.
$Y_t|(U_1,\hdots,U_{t-1})$ is a Gaussian and using the martingale bound in Claim \ref{gaussian_properties},  with $\lambda = 0.9c_j$ we can show that,
\[Pr\left[ \abs{Y_1+\hdots+Y_T} > 0.9c_j \right] \leq 2e^{{-c_j}^2/16}\]
For our choice of $\delta = 0.1$, $Pr[j \in C_T^{disc}] = Pr\left[ \abs{Y_1+\hdots+Y_T} > 0.9c_j \right] \leq 2e^{{-c_j}^2/16}$
Let $Z_j$ be indicator for $j^{th}$ discrepancy constraint being satisfied,
\begin{align*}
    \mathbb{E}[\abs{C_T^{disc}}] 
    &= \mathbb{E}[Z_1+\hdots+Z_m]
    = \mathbb{E}[Z_1] + \hdots + \mathbb{E}[Z_m]\\
    &= Pr[1 \in C_T^{disc}] + \hdots + Pr[m \in C_T^{disc}]
    \leq 2\sum_{j=1}^{m}{e^{{-c_j}^2/16}} \leq 2.\dfrac{n}{16} = \dfrac{n}{8}
\end{align*}
\end{proof}
We next show that, in expectation, a large number of variable constraints is satisfied.
\begin{Claim} \label{expt_distance}
$\mathbb{E}[\norm{X_T}^2 \leq n]$
\end{Claim}
\begin{Claim} \label{variable_constraint_bound}
$\mathbb{E}[\abs{C_T^{var}}] \geq 0.56n$
\end{Claim}
If an update has a large $\abs{C_t^{var}}$ it helps with the Claim \ref{variable_constraint_bound} while if $\abs{C_t^{var}}$ is small, there is a lot of freedom in null space and by Claim \ref{gaussian_properties} $\E\left[\norm{X_t}^2\right]$ increases significantly which cannot happen too often due to Claim \ref{expt_distance}
\begin{proof}[Proof of Lemma \ref{pcl}]
\begin{align*}
  \mathbb{E}[\abs{C_T^{var}}] 
&\leq nPr\left[\abs{C_T^{var}} \geq \dfrac{n}{2}\right] + \dfrac{n}{2}\paren{1- Pr\left[\abs{C_T^{var}} \geq \dfrac{n}{2}\right]}\\
&\leq \dfrac{n}{2}Pr\left[\abs{C_T^{var}} \geq \dfrac{n}{2}\right]  + \dfrac{n}{2}
\end{align*}
Using Claim \ref{variable_constraint_bound}, we get that $Pr\left[\abs{C_T^{var}} > \dfrac{n}{2}\right] > 0.12 (> 0.1)$
\end{proof}
\begin{theorem}
For a set system $\paren{V,\mathcal{S}}$ there exists a polynomial time randomized algorithm running in time $O((m+n)^3)$ that computes a coloring $\chi$ such that disc$\paren{\mathcal{S}} < K\sqrt{n\log_2(m/n)}$ for a constant $K$.
\end{theorem}
\begin{proof}
Let $\delta = 1/n^2$ and $c_j = \alpha(m,n) = 8 \sqrt{\log(m/n)}$ (to satisfy conditions for Lemma \ref{pcl}) and set $x_0 =0^n$ and apply Lemma \ref{pcl} to obtain a vector $x_1 \in [-1,1]^n$ which satisfies 
$\abs{\inprod{v_i,x_1}} \leq 8\sqrt{n}\sqrt{\log(m/n)}$ and $\abs{\set{i: \abs{x_i} \geq 1 - \delta}} \geq n/2$ with probability 0.1 (can be boosted).\par
The variables which satisfy variable constraint are fixed and the corresponding columns is removed from the incidence matrix of $(V,\mathcal{S})$ to obtain a new set of vectors restricted to the remaining variables ($I_t$) as ${v_i}_{\mkern 1mu \vrule height 1ex\mkern2mu {I_t}}$ and start the whole procedure again with ${x_1}_{\mkern 1mu \vrule height 1ex\mkern2mu {I_1}}$.
We need to repeat the procedure for $2\log n$ times so that $\abs{x_i} \geq 1- \delta, \forall i \in [n]$ and $\abs{v_j,x} < K\sqrt{n\log(m/n)}, \forall j \in [m]$. 

To get a proper coloring we round $x$ as $\chi(i) = sign(x_i) $,
\[ \inprod{v_j,\chi} = \inprod{v_j,x} \pm \delta n \leq K\sqrt{n\log(m/n)} + 1/n \leq (K+1)\sqrt{n\log(m/n)} \]
\end{proof}
