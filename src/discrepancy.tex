\section{Discrepancy Minimization}

A fundamental problem in combinatorics is minimizing the discrepancy of a set system.
Given a set system $\paren{\mathcal{S} = \set{S_1,S_2,\hdots,S_m}}$, formed by taking subsets of universe $\paren{V = \set{1,2,\hdots,n}}$, we define a coloring $\chi: V \to \set{-1,+1}$.
The minimum discrepancy of the set system $\paren{\disc\paren{\mathcal{S}}}$ is defined as
\[ \disc\paren{\mathcal{S}} = \min_{\chi}\max_{S \in \mathcal{S}}{\abs{\sum_{i \in S}{\chi(i)}}} \]
This can be generalised to define discrepancy for arbitrary real matrices $A$ (here we consider $A$ as the incidence matrix of $\paren{V,S}$) as
\[ \disc(A) = \min_{x \in \set{-1,+1}^n} \norm{Ax}_{\infty}\]

For a random coloring, the resulting discrepancy can be shown to be bounded by $3\sqrt{n\log m}$ using Chernoff and Union bounds.
In \cite{Spencer1985}, Spencer showed that for any set system $\mathcal{S}$, there exists a coloring $\chi$ such that $\disc(\mathcal{S}) < K\sqrt{n\;\log_2{(m/n)}}$.
For $m=n$ (our regime of interest from now onwards), we can set $K=6$ and we get a significant improvement over random coloring.
In fact, this is tight if we consider the incidence matrix of $\paren{V,S}$ to be a Hadamard matrix. The proof is non-constructive and it was conjectured by Spencer that no efficient algorithm exists for finding such a coloring.

In a breakthrough result \cite{bansal10cadm}, Nikhil Bansal gave the first polynomial time randomized algorithm which gives a coloring with discrepancy $O(\sqrt{n})$.
The algorithm is based on SDP relaxation of the problem and rounding using random walks.
Shachar Lovett and Raghu Meka \cite{12lovettmeka} gave a simpler randomized polynomial time algorithm based on basic linear algebra and a restricted form of random walk called edge walk.
We discuss the result by Lovett and Meka here.

We state a few properties about Gaussians (without proof).
\begin{Claim} \label{gaussian_properties}
Let $\mathcal{N}\paren{\mu,{\sigma}^2}$ denote a Gaussian distribution with mean $\mu$ and variance ${\sigma}^2$. Let $\set{v_1,v_2,\hdots,v_d}$ be an orthonormal basis for a linear subspace $V \subseteq \mathbb{R}^n$.
The standard multi-dimensional Gaussian supported on $V$ is given by $G \sim \mathcal{N}(V) = G_1v_1 + \hdots + G_dv_d$ where $G_1,G_2,\hdots,G_d \sim \mathcal{N}(0,1)$
(it can be proven that the distribution of $G$ does not dependent on the choice of the orthonormal basis).
We have that,\\
(i) For all $u \in \mathbb{R}^n, \inprod{G,u} \sim \mathcal{N}(0,{\sigma}^2)$ where ${\sigma}^2 \leq \norm{u}^2$\\
(ii) Let $\inprod{G,e_i} \sim \mathcal{N}(0,{\sigma_i}^2)$.
Then $\sum_{i=1}^{n}{\sigma_i}^2 = \dim(V)$\\
(iii) For $G \sim \mathcal{N}(0,1), \lambda >0, \Pr\left[\abs{G}>\lambda\right] \leq 2e^{-{\lambda}^2/2} $\\
(iv) Let $\set{X_i}_{i=1}^{T}$ and $\set{Y_i}_{i=1}^{T}$ be a set of random variables
such that $Y_i$ is a function of $X_i$.
If for all $1 \leq i \leq T,x_1,\hdots,x_{i-1}\in \mathbb{R}$ and for $\lambda > 0$,
$Y_i | \paren{X_1 = x_1, X_2=x_2, \hdots, X_{i-1}=x_{i-1}}$
is a Gaussian with 0 mean and variance at most 1, then
\[ \Pr\left[\abs{Y_1 + \hdots + Y_T} \geq \lambda\sqrt{T}\right] \leq 2e^{-{\lambda}^2/2} \]
\end{Claim}

The algorithm rests on a Partial Coloring Lemma \ref{pcl}. Partial Coloring Method was developed by Beck \cite{Beck1981} by relaxing the discrepancy vector to $\set{-1,0,+1}$. Here it is further relaxed to $\left[-1,+1\right]$.
We denote the relaxed discrepancy vector by $x$.

\begin{lemma} [Partial Coloring Lemma]\label{pcl}
Let $v_1,\hdots,v_m \in \mathbb{R}^n$ be vectors and $c_1,\hdots,c_m \geq 0$ be constants such that
$\sum_{j=1}^{m}e^{{-c_j}^2/16} \leq n/16$.
Let $0 < \delta \le 0.1\min_{j=1}^m c_j$ be a small approximation parameter.
There exists a $O\paren{(m+n)^3{\delta}^{-2}\log(mn/\delta)}$-time randomized algorithm
which starting at $x_0 \in [-1,1]^n$ returns the vector $x \in [-1,1]^n$
with probability at least $0.1$, such that\\
\qquad (i) $\forall j, \abs{\inprod{x-x_0,v_j}} \leq c_j\norm{v_j}$ \qquad \qquad \qquad \qquad
(ii) $\abs{\set{i: \abs{x_i} \geq 1- \delta}} \geq n/2$
\end{lemma}

We normalize all $\norm{v_i}=1$ and consider the polytope $\mathcal{P}$ for feasible $x$ as
\[ \mathcal{P} = \set{x\in \mathbb{R}^n : \abs{x_i} \leq 1 \wedge \abs{\inprod{x-x_0,v_j}} \leq c_j }\]
where the constraints are called variable constraints and discrepancy constraints respectively.
We consider a constrained random walk in $\mathcal{P}$ called Edge-walk starting at $x=x_0$
and run it for $T$ steps. Let $\gamma >0$ be the step size of the walk such that
$\delta = O(\gamma \sqrt{\log(nm/\gamma)})$. Let $T = \frac{16}{3\gamma^2}$.
For $t= 1,\hdots,T$ we define,\\
(i) $\Cvar_t := \Cvar_t\paren{X_{t-1}} = \set{ i \in [n] : \abs{\paren{X_{t-1}}_i} \geq 1- \delta}$ \\
(ii) $\Cdisc_t := \Cdisc_t\paren{X_{t-1}} = \set{j \in [m] : \abs{\inprod{X_{t-1}-x_0,v_j}} \geq c_j - \delta}$ \\
(iii) $\mathcal{V}_t := \mathcal{V}\paren{X_{t-1}} = \set{u \in \mathbb{R}^n : \paren{u_i = 0, \forall i \in \Cvar_t} \wedge \paren{\inprod{u,v_j}=0, \forall j \in \Cdisc_t} }$  \\
(iv) $X_t = X_{t-1} + \gamma U_t$, where $U_t \sim \mathcal{N}\paren{\mathcal{V}_t}$

$\Cvar_t$ and $\Cdisc_t$ denote almost satisfied variable and discrepancy constraints and $\mathcal{V}_t$ is the subspace orthogonal to the constraints that have been satisfied.
We first observe that the update can only shrink the null space $\mathcal{V}_t$ at each iteration.
\begin{Claim}
$\Cvar_t \subseteq \Cvar_{t+1}$, $\Cdisc_t \subseteq \Cdisc_{t+1}$ and $\dim(\mathcal{V}_t) \geq \dim(\mathcal{V}_{t+1})$ $\forall$ $1 \leq t \leq T$
\end{Claim}

Next we claim that this edge walk remains inside $\mathcal{P}$ with high probability.
\begin{Claim}
For a small step size $\gamma \leq \delta/\sqrt{C\log(mn/\gamma)}$ and large constant $C$, the probability that the walk never leaves $\mathcal{P}$ $\paren{X_0,\hdots,X_T \in \mathcal{P}}$ is at least $1 - 1/(mn)^{\paren{\frac{C}{2}-1}}$.
\end{Claim}
\begin{proof}
Let $E_t$ be the event that the walk leaves $\mathcal{P}$ for the first time.
For this to happen, one of the constraints has to be violated, say $(X_t)_i>1$.
This implies that $(X_{t-1})_i \leq 1 - \delta$ as otherwise $i \in \Cvar_t$ and $(U_t)_i=0$ and $(X_{t})_i=(X_{t-1})_i$.
The above holds if $\abs{(U_t)_i} \geq \delta/\gamma$ which implies that $\abs{\inprod{U_t,w}} \geq \delta/\gamma$ for some $w \in \mathbb{R}^n$.
Since $U_t \sim \mathcal{N}(\mathcal{V}_t)$, by Claim \ref{gaussian_properties}, $\inprod{U_t,w}$ is a standard Gaussian.
Invoking the tail bound in Claim \ref{gaussian_properties} gives us the desired result.
\end{proof}

Next we show that on an average, only a few discrepancy constraints are almost satisfied.
\begin{Claim}
\label{thm:less-disc-constraints}
$\E\left[\abs{\Cdisc_T} < n/8 \right]$
\end{Claim}
\begin{proof}
Define $Y_t = \inprod{v_j,U_t}$. Since $\delta \le 0.1 c_j$ for all $j$, $\Pr[j \in \Cdisc_T]$
\[ = \Pr\left[ \abs{\inprod{v_j, X - x_0}} \ge c_j - \delta \right]
\le \Pr\left[ \abs{\inprod{v_j, X - x_0}} \ge 0.9c_j \right]
= \Pr\left[ \abs{\sum_{t=1}^T Y_t} \ge \frac{0.9c_j}{\gamma} \right] \]
$Y_t|(U_1,\hdots,U_{t-1})$ is a Gaussian with mean 0 and variance at most 1 and is a function of $U_t$.
So apply Claim \ref{gaussian_properties}(iv) to get
\[ \Pr\left[ \abs{\sum_{t=1}^T Y_t} \ge \frac{0.9c_j}{\gamma} \right]
\le 2\exp\left(-\frac{0.81 c_j^2}{2\gamma^2T}\right)
= 2\exp\left(-1.215 \frac{c_j^2}{16}\right)
\le 2\exp\left(-\frac{c_j^2}{16}\right) \]
\[ \E[\abs{\Cdisc_T}]
= \sum_{j=1}^m \Pr[j \in \Cdisc_T]
\le 2\sum_{j=1}^m \exp\left(-\frac{c_j^2}{16}\right)
\le 2\frac{n}{16} = \frac{n}{8} \]
\end{proof}

We next show that, in expectation, a large number of variable constraints is satisfied.
\begin{Claim} \label{expt_distance}
$\E[\norm{X_T}^2 \leq n]$
\end{Claim}
\begin{Claim} \label{variable_constraint_bound}
$\E[\abs{\Cvar_T}] \geq 0.68n$
\end{Claim}
\begin{proof}[Proof sketch]
If $\E[\abs{\Cvar_T}] \ge 0.68n$, then we are done.
If $\E[\abs{\Cvar_T}] < 0.68n$, then by Claim \ref{thm:less-disc-constraints},
$\E[\dim(\mathcal{V}_t)]$ is sufficiently large for all $t$.

By Claim \ref{gaussian_properties}(ii), for all $t$,
$\E[\norm{X_t}_2^2] = \E[\norm{X_{t-1}}_2^2] + \gamma^2\E[\dim(\mathcal{V}_t)]$,
so $\E[\norm{X_t}_2^2]$ increases significantly for all $t$,
which cannot happen by Claim \ref{expt_distance}.
\end{proof}

\begin{proof}[Proof of Lemma \ref{pcl}]
\begin{align*}
\E[\abs{\Cvar_T}]
&\leq n\Pr\left[\abs{\Cvar_T} \geq \dfrac{n}{2}\right] + \dfrac{n}{2}\paren{1- \Pr\left[\abs{\Cvar_T} \geq \dfrac{n}{2}\right]}\\
&\leq \dfrac{n}{2}\Pr\left[\abs{\Cvar_T} \geq \dfrac{n}{2}\right]  + \dfrac{n}{2}
\end{align*}
Using Claim \ref{variable_constraint_bound}, we get that $\Pr\left[\abs{\Cvar_T} > \dfrac{n}{2}\right] > 0.36 (> 0.1)$
\end{proof}

\begin{theorem}
For a set system $\paren{V,\mathcal{S}}$ there exists a polynomial time randomized algorithm running in time $O((m+n)^5)$ that computes a coloring $\chi$ such that disc$\paren{\mathcal{S}} < K\sqrt{n\log_2(m/n)}$ for a constant $K$.
\end{theorem}
\begin{proof}[Proof sketch]
Let $\delta = 1/n$ and $c_j = \alpha(m,n) = 4 \sqrt{\log(16m/n)}$.
Set $x_0 =0^n$ and apply Lemma \ref{pcl} to obtain a vector $x_1 \in [-1,1]^n$ which satisfies
$\abs{\inprod{v_i,x_1}} \leq 4\sqrt{n}\sqrt{\log(16m/n)}$
and $\abs{\set{i: \abs{x_i} \geq 1 - \delta}} \geq n/2$ with probability 0.1 (can be boosted).

The variables that satisfy $\abs{x_i} \ge 1-\delta$ are fixed
and the corresponding columns are removed from the incidence matrix of $(V,\mathcal{S})$
to obtain a new set of vectors restricted to the remaining variables
($I_t$) as ${v_j}_{\mkern 1mu \vrule height 1ex\mkern2mu {I_t}}$.
Now recursively apply this algorithm with ${x_1}_{\mkern 1mu \vrule height 1ex\mkern2mu {I_1}}$
in place of $x_0$ and ${v_j}_{\mkern 1mu \vrule height 1ex\mkern2mu {I_t}}$ in place of $v_j$.
Since the number of variables get halved each time,
we need to repeat the procedure at most $\log_2 n$ times.
It can be proven that we finally get
$\abs{x_i} \geq 1 - \delta, \forall i \in [n]$ and
$\abs{\inprod{v_j,x}} < K\sqrt{n\log(16m/n)}, \forall j \in [m]$ for some constant $K$
(proof can be found in section 6 of \cite{12lovettmeka}).

To get a proper coloring, we round $x$ as $\chi(i) = \sign(x_i)$,
\[ \abs{\inprod{v_j,\chi}} \le \abs{\inprod{v_j,x}} + \delta n
\leq K\sqrt{n\log(16m/n)} + 1 \leq (K+1)\sqrt{n\log(16m/n)} \]

By choosing $\delta = 1/(8 \log m)$ and doing randomized rounding on the fractional coloring as in \cite{12lovettmeka} instead of this deterministic rounding, the run time can be reduced to
$\widetilde{O}((m+n)^3)$.
\end{proof}
